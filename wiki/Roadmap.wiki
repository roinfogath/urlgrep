#summary What to do?
#labels Featured,Phase-Design

==v0.5==

*DONE*:
  * ~~Proxy support (via environment variable http_proxy)~~
  * ~~Cookie support (--cookie to give the cookie)~~
  * ~~Fixing bugs in URL constructions~~

==v1.0==

TODO list:
  * ~~Full HTTPS support (seems to be buggy in some cases)~~ look like it works
  * ~~Handle the `<base href="..." />` for link constructions~~
  * Clean the code (refactoring)...
  * Implement a callback function that could be called for each URL
  * Regroup similar URL in output
     ** for example _index.php?id=1_, _index.php?id=2_, ..., _index.php?id=42_ to become only _index.php?id=*X*_)
  * Check the HTTP header to make sure we are not requesting a binary file
  * Generate random User Agents

==v2.0==

TODO list:
  * Handle redirections
  * Colorize the results (like grep --color)
  * Stealth mode
  * HTML generation
  * Parallelize HTTP requests using threads for faster results
  * Maybe an even more generic code and provide a simple crawler API, then copy our code in it (because the crawler could be useful in many other cases)